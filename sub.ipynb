{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import spacy\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_samples, skip_window, data_index, data): \n",
    "    assert batch_size % num_samples == 0\n",
    "    assert num_samples <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # span is the width of the sliding window\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span]) # initial buffer content = first sliding window\n",
    "    \n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_samples):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        random.shuffle(context_words)\n",
    "        words_to_use = collections.deque(context_words) # now we obtain a random list of context words\n",
    "        for j in range(num_samples): # generate the training pairs\n",
    "            batch[i * num_samples + j] = buffer[skip_window]\n",
    "            context_word = words_to_use.pop()\n",
    "            labels[i * num_samples + j, 0] = buffer[context_word] # buffer[context_word] is a random context word\n",
    "        \n",
    "        # slide the window to the next position    \n",
    "        if data_index == len(data):\n",
    "            buffer = data[:span]\n",
    "            data_index = span\n",
    "        else: \n",
    "            buffer.append(data[data_index]) # note that due to the size limit, the left most word is automatically removed from the buffer.\n",
    "            data_index += 1\n",
    "                \n",
    "    # end-of-for\n",
    "    data_index = (data_index + len(data) - span) % len(data) # move data_index back by `span`\n",
    "    return batch, labels, data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(input_dir, vocabulary_size):\n",
    "    ## Phase 1: Read files\n",
    "    data = []\n",
    "    with zipfile.ZipFile(input_dir) as f:\n",
    "        file_list = list(filter(lambda x: x[0] != \"_\" and \".txt\" in x, f.namelist()))\n",
    "        for file_name in file_list:\n",
    "            data += tf.compat.as_str(f.read(file_name)).split()\n",
    "\n",
    "    ## Phase 2: Data preprocess\n",
    "    nlp = spacy.load('en')\n",
    "    docs = nlp(' '.join(data))\n",
    "    words = []\n",
    "    for tok in docs:\n",
    "        # For each word in the corpus, if it is an adjective, instead of using it's existing context we now take advantage of the dependency\n",
    "        # parser of Spacy to use all of it's ancestor as it's new context  \n",
    "        if tok.pos_ == \"ADJ\":\n",
    "            context = [anc.lemma_ for anc in tok.ancestors][:2]\n",
    "            if len(context) == 0:\n",
    "                before_, next_ = docs[tok.i-1], docs[tok.i+1]\n",
    "                context = [before_.lemma_, next_.lemma_]\n",
    "            context.insert(len(context) // 2, tok.lower_)\n",
    "            for w in context:\n",
    "                words.append(w)\n",
    "    \n",
    "    ## Phase 3: Build dataset\n",
    "#     vocabulary_size = 50000\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        data.append(index)\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    ## Phase 4: Write dataset to file\n",
    "    pickle.dump([data, dictionary, reversed_dictionary], open(\"data_set.p\", \"wb\"))\n",
    "# process_data(\"./BBC_Data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjective_embeddings(data_file, embeddings_file_name, num_steps, embedding_size, batch_size, skip_window, num_samples, l_rate, num_sampled):\n",
    "    # Load dataset\n",
    "    with open(data_file, \"rb\") as file:\n",
    "        data, dictionary, reversed_dictionary = pickle.load(file)\n",
    "        \n",
    "    # Specification of Training data:\n",
    "#     batch_size = 128\n",
    "#     skip_window = 1\n",
    "#     num_samples = 2\n",
    "#     num_sampled = 64\n",
    "    vocabulary_size = len(dictionary)\n",
    "\n",
    "    # Specification of test sample:\n",
    "    sample_size = 20\n",
    "    sample_window = 100\n",
    "    sample_examples = np.random.choice(sample_window, sample_size, replace=False)\n",
    "    \n",
    "    # Begin training\n",
    "    with tf.Session() as session:\n",
    "        train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "        sample_dataset = tf.constant(sample_examples, dtype=tf.int32)\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)            \n",
    "\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                          stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=nce_weights, biases=nce_biases, \n",
    "                                                 labels=train_labels, inputs=embed, \n",
    "                                                 num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "        \n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(loss)\n",
    "\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "\n",
    "        sample_embeddings = tf.nn.embedding_lookup(normalized_embeddings, sample_dataset)\n",
    "        similarity = tf.matmul(sample_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        session.run(init)\n",
    "        average_loss = 0\n",
    "        data_index = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_labels, data_index = generate_batch(batch_size, num_samples, skip_window, data_index, data)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "#             print(step, loss_val)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        \n",
    "        file = open(embeddings_file_name, 'w')\n",
    "        for i in range(len(final_embeddings)):\n",
    "            file.write(reversed_dictionary[i])\n",
    "            for d in final_embeddings[i]:\n",
    "                file.write(' ')\n",
    "                file.write(\"%.6f\" % d)\n",
    "            file.write('\\n')\n",
    "        file.close()\n",
    "                    \n",
    "# adjective_embeddings(\"data_set.p\", \"adjective_embeddings.txt\", 100001, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Compute_topk(model_file, input_adjective, top_k):\n",
    "    gensim.scripts.glove2word2vec.glove2word2vec(model_file, \"word2vec.w2v\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.w2v\", binary=False)\n",
    "    ground_truth = {'new' : ['novel', 'recent', 'first', 'current', 'latest'],\n",
    "                'more': ['higher', 'larger', 'unlimited', 'greater', 'countless'],\n",
    "                'last': ['past', 'final', 'previous', 'earlier', 'late'], \n",
    "                'best': ['finest', 'good', 'greatest', 'spotless', 'perfect'], \n",
    "                'next': ['coming', 'following', 'forthcoming', 'consecutive', 'upcoming'], \n",
    "                'many': ['most', 'countless', 'excess', 'several', 'much'], \n",
    "                'good': ['better', 'best', 'worthwhile', 'prosperous', 'fantastic']}\n",
    "    \n",
    "    count = 0\n",
    "    nlp = spacy.load('en')\n",
    "    for testing_word in ground_truth.keys():\n",
    "        truth = ground_truth[testing_word]\n",
    "        \n",
    "        possible_words = [testing_word] + [i[0] for i in model.wv.most_similar(testing_word, topn=top_k * 100)]\n",
    "        doc = nlp(' '.join(possible_words))\n",
    "        \n",
    "        input_adjective_tag = doc[0].tag_\n",
    "        if input_adjective_tag in ['JJ', 'JJR', 'JJS']:\n",
    "            target_tag = input_adjective_tag\n",
    "        else:\n",
    "            target_tag = 'JJ'\n",
    "        \n",
    "        res = []\n",
    "        i = 1\n",
    "#         print(len(doc), count)\n",
    "        while len(res) < top_k and i < len(doc):\n",
    "            next_word = doc[i]\n",
    "            i += 1\n",
    "            if (next_word.tag_ == target_tag):\n",
    "                res.append(next_word.text)\n",
    "                \n",
    "        for word in res:\n",
    "            if word in truth:\n",
    "#                 print(testing_word, word)\n",
    "                count += 1\n",
    "    print(count)\n",
    "# Compute_topk(\"adjective_embeddings.txt\", 'new', 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "def tmp():\n",
    "    gensim.scripts.glove2word2vec.glove2word2vec(\"adjective_embeddings.txt\", \"word2vec.w2v\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\"word2vec.w2v\", binary=False)\n",
    "    nlp = spacy.load('en')\n",
    "    files = listdir(\"dev_set\")\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for input_adjective in files:\n",
    "        file = open(\"dev_set/\" + input_adjective)\n",
    "        truth = [line.strip() for line in file]\n",
    "        top_k = len(truth)\n",
    "        total += top_k\n",
    "    \n",
    "        possible_words = [input_adjective] + [i[0] for i in model.wv.most_similar(input_adjective, topn=top_k * 100)]\n",
    "        doc = nlp(\" \".join(possible_words))\n",
    "    \n",
    "        input_adjective_tag = doc[0].tag_\n",
    "        if input_adjective_tag in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "            target_tag = input_adjective_tag\n",
    "        else:\n",
    "            target_tag = \"JJ\"\n",
    "        \n",
    "        res = []\n",
    "        i = 0\n",
    "        while len(res) < top_k and i+1 < len(doc):\n",
    "            next_word = doc[i+1]\n",
    "            i += 1\n",
    "            if (next_word.tag_ == target_tag):\n",
    "                res.append(next_word.text)\n",
    "\n",
    "        i = 0\n",
    "        while len(res) < top_k and i+1 < len(doc):\n",
    "            next_word = doc[i+1]\n",
    "            i += 1\n",
    "            if next_word.text not in res and next_word.tag_ in [\"JJ\", \"JJR\", \"JJS\"]:\n",
    "                res.append(next_word.text)\n",
    "\n",
    "\n",
    "\n",
    "        for word in res:\n",
    "            if word in truth:\n",
    "                count += 1\n",
    "            \n",
    "    print(count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09318181818181819\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.0893465909090909\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.0924715909090909\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.0971590909090909\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09232954545454546\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09176136363636364\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.08877840909090909\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09289772727272727\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09105113636363636\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.002\n",
      "0.09019886363636363\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.004\n",
      "0.08678977272727273\n",
      "vocab size: 2000, batch_size: 24, skip_window: 1, p_sample: 2, n_sample: 24, l_rate: 0.004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-01c1b54c5e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab size: {}, batch_size: {}, skip_window: {}, p_sample: {}, n_sample: {}, l_rate: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         \u001b[0madjective_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_set.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"adjective_embeddings.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                         \u001b[0mtmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-aaf8451fa093>\u001b[0m in \u001b[0;36madjective_embeddings\u001b[0;34m(data_file, embeddings_file_name, num_steps, embedding_size, batch_size, skip_window, num_samples, l_rate, num_sampled)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;31m#             print(step, loss_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for vocab_size in [2000]:\n",
    "    process_data(\"./BBC_Data.zip\", vocab_size)\n",
    "    for skip_window, number_sample in [[1, 2]]:\n",
    "        for batch_size in [24]:\n",
    "            for l_rate in [0.002, 0.004, 0.008]:\n",
    "                for n_sample in [24]:\n",
    "                    for _ in range(10): \n",
    "                        print(\"vocab size: {}, batch_size: {}, skip_window: {}, p_sample: {}, n_sample: {}, l_rate: {}\".format(vocab_size, batch_size, skip_window, number_sample, n_sample, l_rate))\n",
    "                        adjective_embeddings(\"data_set.p\", \"adjective_embeddings.txt\", 100001, 200, batch_size, skip_window, number_sample, l_rate, n_sample)\n",
    "                        tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
